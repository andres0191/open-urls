# ğŸ•·ï¸ Open URLs Crawler

## Overview
**Open URLs Crawler** is a Python-based web scraper designed to extract all unique hyperlinks from a given website.  
It intelligently filters out duplicate links that share the same root domain.

This tool combines the power of **Selenium** for JavaScript-rendered pages and **BeautifulSoup** for parsing static HTML content.  
The final output is neatly saved into an Excel file for easy review and data management.
---

## ğŸš€ Features

- Extracts all unique hyperlinks (`<a href="...">`) from any website.
- Supports both **JavaScript-heavy** and **static HTML** sites.
- Exports results automatically to an **Excel (.xlsx)** file.
- User can input the target URL directly from the console.
---

## ğŸ§° Technologies Used

| Technology | Description |
|-------------|-------------|
| **Python 3.14** | Main programming language used to build the crawler. |
| **Selenium** | Automates web browsers to load dynamic or JavaScript-rendered content. |
| **BeautifulSoup4** | Parses and extracts links from static HTML pages. |
| **Requests** | Handles standard HTTP requests for faster static page access. |
| **OpenPyXL** | Creates and writes the extracted results into Excel files. |
| **Chrome WebDriver** | Provides browser automation through Selenium for Chrome. |
| **Git** | Version control and project management. |
| **Visual Studio Code** | Development environment used to write and test the code. |
---

## ğŸ§© How It Works

1. The script prompts the user to input a website URL.
2. Selenium loads the website in headless mode (without opening the browser window).
3. BeautifulSoup parses the HTML content.
4. The script extracts all `<a>` tags with valid `href` attributes.
5. Duplicates are removed by comparing domain roots (e.g., `https://example.com/` is kept only once).
6. Links containing blocked domains (e.g., Google, YouTube, Facebook, etc.) are skipped.
7. The clean list of unique URLs is exported to `output_links.xlsx`.
---

## âš™ï¸ Installation & Setup

1. **Clone the repository:**
   ```bash
   git clone git@github.com:andres0191/open-urls.git
   cd open-urls
Install dependencies:

bash
pip install selenium beautifulsoup4 requests openpyxl
Make sure you have Chrome and ChromeDriver installed.
Selenium will automatically connect to the ChromeDriver executable.

â–¶ï¸ Usage
Run the script from the terminal:

bash
Copiar cÃ³digo
python crawler_selenium.py
Then enter the URL you want to analyze (for example):

mathematica

Enter the URL to crawl: https://www.mercadolibre.com.co/
The results will be saved in:

output_links.xlsx

ğŸ§  Future Improvements
Add asynchronous support using aiohttp for faster crawling.
Include multi-page scraping with pagination detection.
Add a graphical interface (GUI) for non-technical users.
Support for exporting results in CSV or JSON formats.

ğŸ“„ License
This project is open-source and available under the MIT License.

ğŸ‘¨â€ğŸ’» Author: [AndrÃ©s Felipe Garcia Rendon](https://linkedin.com/in/anfegar)

ğŸ“‚ GitHub: [GitHub @andres0191](https://github.com/andres0191)

âœ‰ï¸ Email: [email ](mailto:felipe.garcia0191@gmail.com)



